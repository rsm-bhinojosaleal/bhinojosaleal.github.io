[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Your Name",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nBidkar Hinojosa Leal\n\n\nApr 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nBidkar Hinojosa Leal\n\n\nMay 5, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nYour Name\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation Methods\n\n\n\n\n\n\nYour Name\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\nfile_path = 'karlan_list_2007.dta'\ndf = pd.read_stata(file_path)\nprint(df)\n\npd.set_option('display.max_columns', None)\ndf.describe(include='all')\n\n#66% of the data is treatment and 33% is control\n#The mean amount is less than 1 \n#The number of prior donations is 8\n#Only 27% of the population is female\n#pblack and hhmedianincome have the most NaN values in the dataset\n\nnan_counts = df.isna().sum()\nnan_counts\n\n       treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0              0        1  Control       0       0   Control       0       0   \n1              0        1  Control       0       0   Control       0       0   \n2              1        0        1       0       0  $100,000       0       0   \n3              1        0        1       0       0  Unstated       0       0   \n4              1        0        1       0       0   $50,000       0       1   \n...          ...      ...      ...     ...     ...       ...     ...     ...   \n50078          1        0        1       0       0   $25,000       1       0   \n50079          0        1  Control       0       0   Control       0       0   \n50080          0        1  Control       0       0   Control       0       0   \n50081          1        0        3       0       1  Unstated       0       0   \n50082          1        0        3       0       1   $25,000       1       0   \n\n       size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0            0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1            0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2            1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3            0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4            0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n...        ...     ...  ...    ...      ...       ...       ...        ...   \n50078        0       0  ...    0.0      1.0  0.872797  0.089959   0.257265   \n50079        0       0  ...    0.0      1.0  0.688262  0.108889   0.288792   \n50080        0       0  ...    1.0      0.0  0.900000  0.021311   0.178689   \n50081        0       1  ...    1.0      0.0  0.917206  0.008257   0.225619   \n50082        0       0  ...    0.0      1.0  0.530023  0.074112   0.340698   \n\n       ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0           2.10          28517.0  0.499807      0.324528       1.000000  \n1            NaN              NaN       NaN           NaN            NaN  \n2           2.48          51175.0  0.721941      0.192668       1.000000  \n3           2.65          79269.0  0.920431      0.412142       1.000000  \n4           1.85          40908.0  0.416072      0.439965       1.000000  \n...          ...              ...       ...           ...            ...  \n50078       2.13          45047.0  0.771316      0.263744       1.000000  \n50079       2.67          74655.0  0.741931      0.586466       1.000000  \n50080       2.36          26667.0  0.778689      0.107930       0.000000  \n50081       2.57          39530.0  0.733988      0.184768       0.634903  \n50082       3.70          48744.0  0.717843      0.127941       0.994181  \n\n[50083 rows x 51 columns]\n\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\ncontrol_group = df[df['treatment'] == 0]['mrm2']\ntreatment_group = df[df['treatment'] == 1]['mrm2']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['mrm2'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['mrm2']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n# I get the same p value for the treatment which is .905 which means we cannot say with 95% confidence that treatment and control have different number of months since last donation\n\ncontrol_group = df[df['treatment'] == 0]['amount']\ntreatment_group = df[df['treatment'] == 1]['amount']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['amount'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['amount']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n#The p value for .063 make us not reject the null hypothesis so we cant say the amount donated changes from control to treatment.\n\ncontrol_group = df[df['treatment'] == 0]['hpa']\ntreatment_group = df[df['treatment'] == 1]['hpa']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['amount'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['hpa']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n#The highest previous contribution does not change from control to treatment, null hypothesis cannot be rejected.\n\nT-test results -- t-statistic: -0.1194921058159193 p-value: 0.9048859731777738\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.905\nTime:                        18:03:49   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test results -- t-statistic: -1.8608252329783106 p-value: 0.06277471702258718\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.464\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0627\nTime:                        18:03:49   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50082   AIC:                         3.589e+05\nDf Residuals:                   50080   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96858.715   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240721274.363\nSkew:                          15.296   Prob(JB):                         0.00\nKurtosis:                     341.262   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test results -- t-statistic: -0.9467832370069835 p-value: 0.34375381350882095\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hpa   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.8949\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.344\nTime:                        18:03:49   Log-Likelihood:            -2.8467e+05\nNo. Observations:               50082   AIC:                         5.693e+05\nDf Residuals:                   50080   BIC:                         5.694e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     58.9602      0.551    107.005      0.000      57.880      60.040\ntreatment      0.6389      0.675      0.947      0.344      -0.684       1.961\n==============================================================================\nOmnibus:                    66198.015   Durbin-Watson:                   2.003\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         14447952.958\nSkew:                           7.552   Prob(JB):                         0.00\nKurtosis:                      84.826   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['group'] = df['control'].map({0: 'treatment', 1: 'control'})\nproportions = df.groupby('group')['gave'].mean().reset_index()\nproportions.rename(columns={'gave': 'proportion'}, inplace=True)\n\nsns.barplot(x='group', y='proportion', data=proportions)\nplt.title('Proportion of Donations in Treatment and Control Groups')\nplt.ylabel('Proportion of People Who Donated')\nplt.xlabel('Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\n\ncontrol_group = df[df['treatment'] == 0]['gave']\ntreatment_group = df[df['treatment'] == 1]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\\\n\ndf['intercept'] = 1\n\n# Define the model\nmodel = sm.OLS(df['gave'], df[['intercept', 'control']])\nresults = model.fit()\n\nprint(results.summary())\n\n#The results give a p value under .05 which means we can reject the null hypothesis. In a simpler wording, we can conclude that the xcontrol and treatment groups have a significant difference in the percentage of people who donated, meaning there was a higher percentage of people in the treatment group which can be observed in the graph.\n\nT-test results -- t-statistic: -3.101804902209603 p-value: 0.0019245154824258396\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.621\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):            0.00192\nTime:                        18:03:49   Log-Likelihood:                 26629.\nNo. Observations:               50082   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0220      0.001     28.326      0.000       0.021       0.024\ncontrol       -0.0042      0.001     -3.102      0.002      -0.007      -0.002\n==============================================================================\nOmnibus:                    59812.651   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316882.272\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         2.41\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nindependent_vars = df[['intercept', 'control']]\ndependent_var = df['gave']\n\nprobit_model = sm.Probit(dependent_var, independent_vars)\n\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n#The results indicate that being in the control group significantly decreases the likelihood of making a donation compared to the treatment group. \n#The model's overall fit is not particularly strong, suggesting other factors also influence the decision to donate that are not included in the model.\n\nOptimization terminated successfully.\n         Current function value: 0.100445\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50082\nModel:                         Probit   Df Residuals:                    50080\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 16 Apr 2024   Pseudo R-squ.:               0.0009786\nTime:                        18:03:50   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001694\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.0133      0.015   -131.732      0.000      -2.043      -1.983\ncontrol       -0.0868      0.028     -3.113      0.002      -0.141      -0.032\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\ncontrol_group = df[df['ratio']==1]['gave']\ntreatment_group = df[df['ratio']==2]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ncontrol_group = df[df['ratio']==2]['gave']\ntreatment_group = df[df['ratio']==3]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\n#Bteween the ratio 1 to 2 and ratio 2 to 3 there is no difference in the amount given.\n\nT-test results -- t-statistic: -0.9660455010162683 p-value: 0.33403190645353387\nT-test results -- t-statistic: -0.04909504833709768 p-value: 0.9608439899712724\n\n\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\ndf['ratio1'] = df['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\nX = df[['intercept', 'ratio1', 'ratio2', 'ratio3']]  # Predictor variables\ny = df['gave']  # Dependent variable\n\nols_model = sm.OLS(y, X)\n\n# Fit the model\nresults = ols_model.fit()\nprint(results.summary())\n\n#Intercept (0.0179): This represents the expected value of gave when all the predictor variables (ratio1, ratio2, ratio3) are 0. The intercept is statistically significant (p &lt; 0.0001), indicating a non-zero baseline level of the dependent variable.\n#Ratio1 (0.0029): The coefficient of ratio1 is 0.0029, suggesting that a one-unit increase in ratio1 is associated with an increase of 0.0029 in the expected value of gave, holding other variables constant. However, this effect is not statistically significant at the typical 0.05 level (p = 0.097), indicating that changes in ratio1 may not reliably predict changes in gave.\n#Ratio2 (0.0048): The coefficient for ratio2 is 0.0048, which is statistically significant (p = 0.006). This suggests that a one-unit increase in ratio2 is associated with an increase of 0.0048 in gave, all else being equal. The result is likely to be a reliable predictor of gave.\n#Ratio3 (0.0049): Similar to ratio2, ratio3 shows a statistically significant positive effect on gave (p = 0.005). Each one-unit increase in ratio3 leads to an increase of 0.0049 in the expected value of gave\n#The standard errors are similar to the ones on the papaer which means The standard error here is almost as large as the coefficient itself (0.0029), indicating a lower level of precision. \n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.666\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0117\nTime:                        18:03:50   Log-Likelihood:                 26629.\nNo. Observations:               50082   AIC:                        -5.325e+04\nDf Residuals:                   50078   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.224      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.746      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59811.123   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316422.352\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.437   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\ncoef_ratio1 = 0.0029  # effect of 1:1\ncoef_ratio2 = 0.0048  # effect of 2:1\ncoef_ratio3 = 0.0049  # effect of 3:1\n\n# Calculate differences in effects from regression coefficients\ncoef_diff_1to1_2to1 = coef_ratio2 - coef_ratio1\ncoef_diff_2to1_3to1 = coef_ratio3 - coef_ratio2\n\nprint(f\"Coefficient difference between 1:1 and 2:1 is {coef_diff_1to1_2to1}\")\nprint(f\"Coefficient difference between 2:1 and 3:1 is {coef_diff_2to1_3to1}\")\n\nCoefficient difference between 1:1 and 2:1 is 0.0018999999999999998\nCoefficient difference between 2:1 and 3:1 is 0.00010000000000000026\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\ncontrol_group = df[df['treatment'] == 0]['amount']\ntreatment_group = df[df['treatment'] == 1]['amount']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n#The difference could be alsmot significznt with 95% confidence howeveer it isnt.\n\nT-test results -- t-statistic: -1.8608252329783106 p-value: 0.06277471702258718\n\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\ncontrol_group = df[(df['treatment'] == 0) & (df['gave'] == 1)]['amount']\ntreatment_group = df[(df['treatment'] == 1) & (df['gave'] == 1)]['amount']\n\n# Drop any NaN values from the amount column for both groups\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\n\n# Print the t-test results\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\n#Once the gave column is filtered to only times there was a donation the diference is non existent between control and treatment.\n\nT-test results -- t-statistic: 0.5808388615237938 p-value: 0.5614758782284279\n\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nfig, ax = plt.subplots()\nax.hist(df[(df['amount'] &gt; 0) & (df['treatment'] == 0)]['amount'], bins=50)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Donation Amounts for Control Group')\nplt.show()\n\nfig, ax = plt.subplots()\nax.hist(df[(df['amount'] &gt; 0) & (df['treatment'] == 1)]['amount'], bins=50)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Donation Amounts for Treatment Group')\nplt.show()"
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = df[df['treatment'] == 0]['gave'].mean()\ntreatment = df[df['treatment'] == 1]['gave'].mean()\n\ncontrol_s = bernoulli.rvs(p = control, size = 10000)\ntreatment_s = bernoulli.rvs(p = treatment, size = 10000)\n\ncum_avg = np.cumsum(treatment_s - control_s) / np.arange(1, 10001)\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Draws')\nax.set_title('Cumulative Average')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects/project3/hw2_questions.html",
    "href": "projects/project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndf_blueprinty = pd.read_csv('blueprinty.csv')\ndf_blueprinty\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1495\n1366\n2\nNortheast\n18.5\n1\n\n\n1496\n619\n3\nSouthwest\n22.5\n0\n\n\n1497\n826\n4\nSouthwest\n17.0\n0\n\n\n1498\n601\n3\nSouth\n29.0\n0\n\n\n1499\n602\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 5 columns\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nmean_customers = df_blueprinty[df_blueprinty['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df_blueprinty[df_blueprinty['iscustomer'] == 0]['patents'].mean()\nprint(\"Mean patents for customers:\", mean_customers)\nprint(\"Mean patents for non-customers:\", mean_non_customers)\n\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n\n# Customers\ndf_blueprinty[df_blueprinty['iscustomer'] == 1]['patents'].hist(ax=axs[0], bins=20, color='blue', alpha=0.7)\naxs[0].set_title('Histogram of Patents (Customers)')\naxs[0].set_xlabel('Number of Patents')\naxs[0].set_ylabel('Frequency')\n\n# Non-Customers\ndf_blueprinty[df_blueprinty['iscustomer'] == 0]['patents'].hist(ax=axs[1], bins=20, color='red', alpha=0.7)\naxs[1].set_title('Histogram of Patents (Non-Customers)')\naxs[1].set_xlabel('Number of Patents')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\ntotal_patents = df_blueprinty.groupby('iscustomer')['patents'].sum()\n\ntotal_patents.plot(kind='bar', color=['red', 'blue'])\nplt.title('Total Number of Patents by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customers, 1 = Customers)')\nplt.ylabel('Total Patents')\nplt.xticks(ticks=[0, 1], labels=['Non-Customers', 'Customers'], rotation=0)  \nplt.show()\n\n\n# Based on the visualizations it is clear that there are a lot more patents for non customers as well lie on 3 patents vs the customers whose distribution is along the 4 patents as a mean which is validated by the means calculated.\n\nMean patents for customers: 4.091370558375634\nMean patents for non-customers: 3.6231772831926325\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\nage_by_region_customer = df_blueprinty.groupby(['region', 'iscustomer'])['age'].mean().unstack()\n\n# Bar chart to compare average ages in each region by customer status\nage_by_region_customer.plot(kind='bar', figsize=(10, 6))\nplt.title('Average Age by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Average Age')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status', labels=['Non-Customers', 'Customers'])\nplt.show()\n\n#In all regions the non customers have  a higher age to the customers with northwest having the most difference.\n\ncustomer_count_by_region = df_blueprinty.groupby(['region', 'iscustomer']).size().unstack()\n\n# Plotting the results\ncustomer_count_by_region.plot(kind='bar', figsize=(10, 6))\nplt.title('Customer Count by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status', labels=['Non-Customers', 'Customers'])\nplt.show()\n\n#For the amount of customers no customers in each region it is clear that the most promising region is northeast as it has the most customers but also non customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nLikelihood = ℓ(λ)=∑i=1n​(yi​log(λ)−λ−log(yi​!))\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\n\nimport numpy as np\nfrom scipy.special import factorial, gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        raise ValueError(\"Lambda must be greater than 0\")\n    \n    # Using gammaln to compute the log factorial to avoid overflow\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n    return log_likelihood\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nlog_likelihoods = [poisson_loglikelihood(lambda_, df_blueprinty['patents']) for lambda_ in lambda_values]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, color='blue', linestyle='-', marker='None', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nfrom scipy.optimize import minimize\n\ndef negative_poisson_loglikelihood(lambda_, Y):\n    return -poisson_loglikelihood(lambda_, Y)  # Return negative log likelihood\n\nfrom scipy.optimize import minimize\n\n# Example observed data\nY = df_blueprinty['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.mean(Y)\n\n# Optimization to find the MLE for lambda\nresult = minimize(\n    fun=negative_poisson_loglikelihood,  # function to minimize\n    x0=[initial_lambda],  # initial guess\n    args=(Y,),  # additional arguments for the function\n    method='L-BFGS-B',  # optimization method suitable for bounded problems\n    bounds=[(0.001, None)]  # lambda must be positive, setting a lower bound\n)\n\n# Check the result\nif result.success:\n    mle_lambda = result.x[0]\n    print(f\"MLE for lambda: {mle_lambda}\")\nelse:\n    print(\"Optimization failed:\", result.message)\n\nMLE for lambda: 3.6846666675617414\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Compute the log likelihood for the Poisson regression model.\n    \n    Args:\n    beta (numpy.ndarray): Parameter vector.\n    Y (numpy.ndarray): Observed counts.\n    X (numpy.ndarray): Design matrix of covariates.\n\n    Returns:\n    float: The log likelihood value.\n    \"\"\"\n    # Calculate lambda for each observation\n    linear_combination = X @ beta\n    lambda_i = np.exp(np.clip(linear_combination, None, 20))\n    \n    # Compute the log likelihood\n    log_likelihood = np.sum(Y * linear_combination - lambda_i - gammaln(Y + 1))\n    return log_likelihood\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Add the age squared term\ndf_blueprinty['age_squared'] = df_blueprinty['age'] ** 2\n\n# One-hot encode the 'region' variable, dropping the first category\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df_blueprinty[['region']]).toarray()\n\n# Prepare the design matrix X\n# Adding intercept column with ones\nX = np.hstack([np.ones((df_blueprinty.shape[0], 1)), df_blueprinty[['age', 'age_squared', 'iscustomer']].values, region_encoded])\n\n# Prepare the response variable Y\nY = df_blueprinty['patents'].values\n\ndef negative_poisson_regression_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimize the negative log likelihood\nresult = minimize(\n    fun=negative_poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',  # Bounded optimization\n    bounds=[(None, None)] * X.shape[1]  # No upper bounds, but ensuring no negative infinities\n)\n\nif result.success:\n    print(\"Optimized beta:\", result.x)\nelse:\n    print(\"Optimization failed:\", result.message)\n\nOptimized beta: [-0.00019589  0.11329599 -0.0023141   0.0608041   0.08322599 -0.04022632\n -0.00133076 -0.00568701]\n\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\nimport statsmodels.api as sm\n\n# Assuming df_blueprinty and X, Y are already prepared and include the necessary transformations and dummy variables\nX = sm.add_constant(X)  # Add an intercept to the model, if not already included\n\n# Create the Poisson model object\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\npoisson_results = poisson_model.fit()\n\n# Print the summary of the regression results\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Mon, 29 Apr 2024   Deviance:                       2178.8\nTime:                        21:29:23   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4513      0.184     -2.458      0.014      -0.811      -0.091\nx1             0.1445      0.014     10.414      0.000       0.117       0.172\nx2            -0.0029      0.000    -11.131      0.000      -0.003      -0.002\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0986      0.042      2.347      0.019       0.016       0.181\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0572      0.053      1.085      0.278      -0.046       0.160\nx7             0.0513      0.047      1.088      0.277      -0.041       0.144\n==============================================================================\n\n\ntodo: Interpret the results. What do you conclude about the effect of Blueprinty’s software on patent success?\nThe coefficient for the variable representing whether a firm is a customer of Blueprinty (x3) is positive (0.1181) and statistically significant (p = 0.002). This suggests that firms using Blueprinty’s software have a higher expected count of patents, compared to firms that do not use the software. Specifically, the effect size implies that using Blueprinty’s software increases the expected count of patents by approximately 12.5% (exp(0.1181) ≈ 1.125). This is a robust statistical confirmation supporting the marketing team’s claim that Blueprinty’s software enhances the success rate of patent applications."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndf_blueprinty = pd.read_csv('blueprinty.csv')\ndf_blueprinty\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1495\n1366\n2\nNortheast\n18.5\n1\n\n\n1496\n619\n3\nSouthwest\n22.5\n0\n\n\n1497\n826\n4\nSouthwest\n17.0\n0\n\n\n1498\n601\n3\nSouth\n29.0\n0\n\n\n1499\n602\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 5 columns\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nmean_customers = df_blueprinty[df_blueprinty['iscustomer'] == 1]['patents'].mean()\nmean_non_customers = df_blueprinty[df_blueprinty['iscustomer'] == 0]['patents'].mean()\nprint(\"Mean patents for customers:\", mean_customers)\nprint(\"Mean patents for non-customers:\", mean_non_customers)\n\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n\n# Customers\ndf_blueprinty[df_blueprinty['iscustomer'] == 1]['patents'].hist(ax=axs[0], bins=20, color='blue', alpha=0.7)\naxs[0].set_title('Histogram of Patents (Customers)')\naxs[0].set_xlabel('Number of Patents')\naxs[0].set_ylabel('Frequency')\n\n# Non-Customers\ndf_blueprinty[df_blueprinty['iscustomer'] == 0]['patents'].hist(ax=axs[1], bins=20, color='red', alpha=0.7)\naxs[1].set_title('Histogram of Patents (Non-Customers)')\naxs[1].set_xlabel('Number of Patents')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\ntotal_patents = df_blueprinty.groupby('iscustomer')['patents'].sum()\n\ntotal_patents.plot(kind='bar', color=['red', 'blue'])\nplt.title('Total Number of Patents by Customer Status')\nplt.xlabel('Customer Status (0 = Non-Customers, 1 = Customers)')\nplt.ylabel('Total Patents')\nplt.xticks(ticks=[0, 1], labels=['Non-Customers', 'Customers'], rotation=0)  \nplt.show()\n\n\n# Based on the visualizations it is clear that there are a lot more patents for non customers as well lie on 3 patents vs the customers whose distribution is along the 4 patents as a mean which is validated by the means calculated.\n\nMean patents for customers: 4.091370558375634\nMean patents for non-customers: 3.6231772831926325\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\nage_by_region_customer = df_blueprinty.groupby(['region', 'iscustomer'])['age'].mean().unstack()\n\n# Bar chart to compare average ages in each region by customer status\nage_by_region_customer.plot(kind='bar', figsize=(10, 6))\nplt.title('Average Age by Region and Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Average Age')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status', labels=['Non-Customers', 'Customers'])\nplt.show()\n\n#In all regions the non customers have  a higher age to the customers with northwest having the most difference.\n\ncustomer_count_by_region = df_blueprinty.groupby(['region', 'iscustomer']).size().unstack()\n\n# Plotting the results\ncustomer_count_by_region.plot(kind='bar', figsize=(10, 6))\nplt.title('Customer Count by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status', labels=['Non-Customers', 'Customers'])\nplt.show()\n\n#For the amount of customers no customers in each region it is clear that the most promising region is northeast as it has the most customers but also non customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nLikelihood = ℓ(λ)=∑i=1n​(yi​log(λ)−λ−log(yi​!))\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\n\nimport numpy as np\nfrom scipy.special import factorial, gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        raise ValueError(\"Lambda must be greater than 0\")\n    \n    # Using gammaln to compute the log factorial to avoid overflow\n    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n    return log_likelihood\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nlambda_values = np.linspace(0.1, 10, 100)\n\nlog_likelihoods = [poisson_loglikelihood(lambda_, df_blueprinty['patents']) for lambda_ in lambda_values]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_values, log_likelihoods, color='blue', linestyle='-', marker='None', linewidth=2)\nplt.title('Log-Likelihood of Poisson Distribution for Various Lambda')\nplt.xlabel('Lambda (Rate Parameter)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nfrom scipy.optimize import minimize\n\ndef negative_poisson_loglikelihood(lambda_, Y):\n    return -poisson_loglikelihood(lambda_, Y)  # Return negative log likelihood\n\nfrom scipy.optimize import minimize\n\n# Example observed data\nY = df_blueprinty['patents'].values\n\n# Initial guess for lambda\ninitial_lambda = np.mean(Y)\n\n# Optimization to find the MLE for lambda\nresult = minimize(\n    fun=negative_poisson_loglikelihood,  # function to minimize\n    x0=[initial_lambda],  # initial guess\n    args=(Y,),  # additional arguments for the function\n    method='L-BFGS-B',  # optimization method suitable for bounded problems\n    bounds=[(0.001, None)]  # lambda must be positive, setting a lower bound\n)\n\n# Check the result\nif result.success:\n    mle_lambda = result.x[0]\n    print(f\"MLE for lambda: {mle_lambda}\")\nelse:\n    print(\"Optimization failed:\", result.message)\n\nMLE for lambda: 3.6846666675617414\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Compute the log likelihood for the Poisson regression model.\n    \n    Args:\n    beta (numpy.ndarray): Parameter vector.\n    Y (numpy.ndarray): Observed counts.\n    X (numpy.ndarray): Design matrix of covariates.\n\n    Returns:\n    float: The log likelihood value.\n    \"\"\"\n    # Calculate lambda for each observation\n    linear_combination = X @ beta\n    lambda_i = np.exp(np.clip(linear_combination, None, 20))\n    \n    # Compute the log likelihood\n    log_likelihood = np.sum(Y * linear_combination - lambda_i - gammaln(Y + 1))\n    return log_likelihood\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Add the age squared term\ndf_blueprinty['age_squared'] = df_blueprinty['age'] ** 2\n\n# One-hot encode the 'region' variable, dropping the first category\nencoder = OneHotEncoder(drop='first')\nregion_encoded = encoder.fit_transform(df_blueprinty[['region']]).toarray()\n\n# Prepare the design matrix X\n# Adding intercept column with ones\nX = np.hstack([np.ones((df_blueprinty.shape[0], 1)), df_blueprinty[['age', 'age_squared', 'iscustomer']].values, region_encoded])\n\n# Prepare the response variable Y\nY = df_blueprinty['patents'].values\n\ndef negative_poisson_regression_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\ninitial_beta = np.zeros(X.shape[1])\n\n# Minimize the negative log likelihood\nresult = minimize(\n    fun=negative_poisson_regression_loglikelihood,\n    x0=initial_beta,\n    args=(Y, X),\n    method='L-BFGS-B',  # Bounded optimization\n    bounds=[(None, None)] * X.shape[1]  # No upper bounds, but ensuring no negative infinities\n)\n\nif result.success:\n    print(\"Optimized beta:\", result.x)\nelse:\n    print(\"Optimization failed:\", result.message)\n\nOptimized beta: [-0.00019589  0.11329599 -0.0023141   0.0608041   0.08322599 -0.04022632\n -0.00133076 -0.00568701]\n\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\nimport statsmodels.api as sm\n\n# Assuming df_blueprinty and X, Y are already prepared and include the necessary transformations and dummy variables\nX = sm.add_constant(X)  # Add an intercept to the model, if not already included\n\n# Create the Poisson model object\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\n\n# Fit the model\npoisson_results = poisson_model.fit()\n\n# Print the summary of the regression results\nprint(poisson_results.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.9\nDate:                Mon, 29 Apr 2024   Deviance:                       2178.8\nTime:                        21:29:23   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1152\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4513      0.184     -2.458      0.014      -0.811      -0.091\nx1             0.1445      0.014     10.414      0.000       0.117       0.172\nx2            -0.0029      0.000    -11.131      0.000      -0.003      -0.002\nx3             0.1181      0.039      3.035      0.002       0.042       0.194\nx4             0.0986      0.042      2.347      0.019       0.016       0.181\nx5            -0.0201      0.054     -0.374      0.709      -0.126       0.085\nx6             0.0572      0.053      1.085      0.278      -0.046       0.160\nx7             0.0513      0.047      1.088      0.277      -0.041       0.144\n==============================================================================\n\n\ntodo: Interpret the results. What do you conclude about the effect of Blueprinty’s software on patent success?\nThe coefficient for the variable representing whether a firm is a customer of Blueprinty (x3) is positive (0.1181) and statistically significant (p = 0.002). This suggests that firms using Blueprinty’s software have a higher expected count of patents, compared to firms that do not use the software. Specifically, the effect size implies that using Blueprinty’s software increases the expected count of patents by approximately 12.5% (exp(0.1181) ≈ 1.125). This is a robust statistical confirmation supporting the marketing team’s claim that Blueprinty’s software enhances the success rate of patent applications."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#airbnb-case-study",
    "href": "projects/project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\ndf = pd.read_csv('airbnb.csv')\nprint(df.head())\n\nprint(df.info())\n\n# Create dummy variables\ndf_processed = pd.get_dummies(df, columns=['room_type', 'instant_bookable'], drop_first=True)\n\n# Ensure all columns used in X are numeric and fill or drop NAs\ndf_processed.dropna(subset=['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'], inplace=True)\n\n# Explicitly convert types to float if not already (this handles cases where the dtype might be 'object')\ncolumns_to_convert = ['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']\nfor column in columns_to_convert:\n    df_processed[column] = df_processed[column].astype(float)\n\nboolean_columns = ['room_type_Private room', 'room_type_Shared room', 'instant_bookable_t']\nfor col in boolean_columns:\n    df_processed[col] = df_processed[col].astype(int)\n\n# Define the model\nX = df_processed[['bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value'] + boolean_columns]\ny = df_processed['number_of_reviews'].astype(float)\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n\n# Fit the model\ntry:\n    model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n    print(model.summary())\nexcept Exception as e:\n    print(\"Error in fitting model:\", e)\n\n   Unnamed: 0    id  days last_scraped  host_since        room_type  \\\n0           1  2515  3130     4/2/2017    9/6/2008     Private room   \n1           2  2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2           3  3647  3050     4/2/2017  11/25/2008     Private room   \n3           4  3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4           5  4611  3012     4/2/2017    1/2/2009     Private room   \n\n   bathrooms  bedrooms  price  number_of_reviews  review_scores_cleanliness  \\\n0        1.0       1.0     59                150                        9.0   \n1        1.0       0.0    230                 20                        9.0   \n2        1.0       1.0    150                  0                        NaN   \n3        1.0       1.0     89                116                        9.0   \n4        NaN       1.0     39                 93                        9.0   \n\n   review_scores_location  review_scores_value instant_bookable  \n0                     9.0                  9.0                f  \n1                    10.0                  9.0                f  \n2                     NaN                  NaN                f  \n3                     9.0                  9.0                f  \n4                     8.0                  9.0                t  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\nNone\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30160\nModel:                            GLM   Df Residuals:                    30150\nModel Family:                 Poisson   Df Model:                            9\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -5.2900e+05\nDate:                Mon, 29 Apr 2024   Deviance:                   9.3653e+05\nTime:                        21:29:23   Pearson chi2:                 1.41e+06\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.5649\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.5725      0.016    223.215      0.000       3.541       3.604\nbathrooms                    -0.1240      0.004    -33.091      0.000      -0.131      -0.117\nbedrooms                      0.0749      0.002     37.698      0.000       0.071       0.079\nprice                     -1.435e-05    8.3e-06     -1.729      0.084   -3.06e-05    1.92e-06\nreview_scores_cleanliness     0.1132      0.001     75.820      0.000       0.110       0.116\nreview_scores_location       -0.0768      0.002    -47.796      0.000      -0.080      -0.074\nreview_scores_value          -0.0915      0.002    -50.902      0.000      -0.095      -0.088\nroom_type_Private room       -0.0145      0.003     -5.310      0.000      -0.020      -0.009\nroom_type_Shared room        -0.2519      0.009    -29.229      0.000      -0.269      -0.235\ninstant_bookable_t            0.3344      0.003    115.748      0.000       0.329       0.340\n=============================================================================================\n\n\nIntercept (const):\n    Coefficient: 3.57253.5725\n    This value indicates the log of the expected count of reviews when all other variables are held at zero, which isn't practically feasible for most variables but provides a baseline reference.\n\nBathrooms:\n    Coefficient: −0.1240−0.1240\n    Each additional bathroom is associated with a decrease of about 12.4% in the expected count of reviews. This might suggest that larger properties (more bathrooms) are less frequently reviewed, perhaps due to less frequent rentals or higher costs.\n\nBedrooms:\n    Coefficient: 0.07490.0749\n    Each additional bedroom is associated with an increase of about 7.5% in the expected count of reviews, indicating that larger properties in terms of bedrooms tend to have more bookings.\n\nPrice:\n    Coefficient: −0.00001435−0.00001435\n    Each one-dollar increase in price is associated with a very slight decrease in the expected count of reviews, suggesting that higher prices could deter bookings slightly.\n\nReview Scores - Cleanliness:\n    Coefficient: 0.11320.1132\n    Higher cleanliness scores are strongly associated with an increase in the expected count of reviews, indicating that cleaner properties are more likely to be booked and reviewed.\n\nReview Scores - Location:\n    Coefficient: −0.0768−0.0768\n    A higher location score slightly decreases the expected count of reviews, which is counterintuitive unless properties in desirable locations are less frequently available or require less promotion through reviews.\n\nReview Scores - Value:\n    Coefficient: −0.0915−0.0915\n    Higher scores for value decrease the expected count of reviews, suggesting that guests might expect more for their money and may be critical in reviews when their expectations are not met.\n\nRoom Type - Private room:\n    Coefficient: −0.0145−0.0145\n    Listings for private rooms are associated with a slight decrease in the expected count of reviews compared to entire homes/apartments, possibly indicating fewer bookings or less frequent reviews.\n\nRoom Type - Shared room:\n    Coefficient: −0.2519−0.2519\n    Shared rooms show a significant decrease in the expected count of reviews compared to entire homes/apartments, indicating they are much less popular or frequently booked.\n\nInstant Bookable:\n    Coefficient: 0.33440.3344\n    Listings that are instantly bookable are associated with a substantial increase in the expected count of reviews, suggesting higher booking rates due to the convenience of instant booking."
  },
  {
    "objectID": "projects/project4/hw3_questions.html",
    "href": "projects/project4/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/project4/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\nimport pandas as pd\nyogurt_data = pd.read_csv('yogurt_data.csv')\nyogurt_data.head(10)\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n5\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.092\n0.050\n0.079\n\n\n6\n7\n0\n1\n0\n0\n0\n0\n0\n0\n0.103\n0.081\n0.049\n0.079\n\n\n7\n8\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.086\n0.054\n0.079\n\n\n8\n9\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n9\n10\n1\n0\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.050\n0.079\n\n\n\n\n\n\n\nThe dataset yogurt_data contains the following columns:\nid: Consumer identifier\ny1 to y4: Binary variables indicating which yogurt product was chosen by the consumer (1 if chosen, 0 otherwise)\nf1 to f4: Binary variables indicating if the corresponding yogurt product was featured/advertised (1 if featured, 0 otherwise)\np1 to p4: Prices of the corresponding yogurt products in dollars per ounce\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n# Melt the data to long format\nlong_data = pd.melt(yogurt_data, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'],\n                    var_name='product', value_name='chosen')\n\n# Melt the feature and price data\nfeature_data = pd.melt(yogurt_data, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'],\n                       var_name='product_feature', value_name='featured')\nprice_data = pd.melt(yogurt_data, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'],\n                     var_name='product_price', value_name='price')\n\n# Extract product number from the variable names\nlong_data['product'] = long_data['product'].str.extract('(\\d)').astype(int)\nfeature_data['product'] = feature_data['product_feature'].str.extract('(\\d)').astype(int)\nprice_data['product'] = price_data['product_price'].str.extract('(\\d)').astype(int)\n\n# Merge the long_data with feature_data and price_data\nlong_data = long_data.merge(feature_data[['id', 'product', 'featured']], on=['id', 'product'])\nlong_data = long_data.merge(price_data[['id', 'product', 'price']], on=['id', 'product'])\n\n# Create dummy variables for products 1, 2, and 3\nlong_data['yogurt_1'] = (long_data['product'] == 1).astype(int)\nlong_data['yogurt_2'] = (long_data['product'] == 2).astype(int)\nlong_data['yogurt_3'] = (long_data['product'] == 3).astype(int)\n\n# Display the first few rows of the reshaped dataset\nprint(long_data.head())\n\n   id  product  chosen  featured  price  yogurt_1  yogurt_2  yogurt_3\n0   1        1       0         0  0.108         1         0         0\n1   2        1       0         0  0.108         1         0         0\n2   3        1       0         0  0.108         1         0         0\n3   4        1       0         0  0.108         1         0         0\n4   5        1       0         0  0.125         1         0         0\n\n\n\n\nEstimation\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n\n# Define the utility function\ndef utility(params, data):\n    beta_1, beta_2, beta_3, beta_f, beta_p = params\n    utilities = (beta_1 * data['yogurt_1'] +\n                 beta_2 * data['yogurt_2'] +\n                 beta_3 * data['yogurt_3'] +\n                 beta_f * data['featured'] +\n                 beta_p * data['price'])\n    return utilities\n\ndef log_likelihood(params, data):\n    utilities = utility(params, data)\n    data['exp_utilities'] = np.exp(utilities)\n    \n    # Group by consumer id and calculate the denominator for the softmax function\n    data['sum_exp_utilities'] = data.groupby('id')['exp_utilities'].transform('sum')\n    data['probabilities'] = data['exp_utilities'] / data['sum_exp_utilities']\n    \n    # Calculate the log-likelihood\n    data['log_likelihood'] = data['chosen'] * np.log(data['probabilities'])\n    return -data['log_likelihood'].sum()\n\n# Initial parameter values\ninitial_params = [0, 0, 0, 0, 0]\n\n# Optimize the log-likelihood function\nresult = minimize(log_likelihood, initial_params, args=(long_data), method='BFGS')\n\nbeta_1, beta_2, beta_3, beta_f, beta_p = result.x\n\n# Print the results\nprint('Estimated parameters:')\nprint('beta_1:', beta_1)\nprint('beta_2:', beta_2)\nprint('beta_3:', beta_3)\nprint('beta_f:', beta_f)\nprint('beta_p:', beta_p)\n\nEstimated parameters:\nbeta_1: 1.3877518196538263\nbeta_2: 0.6435048469886004\nbeta_3: -3.086114254337535\nbeta_f: 0.48741429672962905\nbeta_p: -37.057869152778295\n\n\n\n\nDiscussion\nWe learn…\nBased on the 3 betas obtained the yogurt one which has the greatest beta is the most preferred and the least preferred is the yogurt 3 as it has a negative value.\n\nintercepts = [beta_1, beta_2, beta_3]\nmost_preferred = max(intercepts)\nleast_preferred = min(intercepts)\n\n# Calculate the dollar benefit (brand value)\ndollar_benefit = (most_preferred - least_preferred) / beta_p\nprint('Dollar benefit between most and least preferred yogurts:', dollar_benefit)\n\n# Step 3: Simulate counterfactual\n# Calculate the market shares with the original prices\ndef predict_market_shares(params, data):\n    utilities = utility(params, data)\n    data['exp_utilities'] = np.exp(utilities)\n    data['sum_exp_utilities'] = data.groupby('id')['exp_utilities'].transform('sum')\n    data['probabilities'] = data['exp_utilities'] / data['sum_exp_utilities']\n    return data.groupby('product')['probabilities'].mean()\n\n# Original market shares\noriginal_market_shares = predict_market_shares(result.x, long_data)\n\n# Increase the price of yogurt 1 by $0.10\nlong_data_cf = long_data.copy()\nlong_data_cf.loc[long_data_cf['product'] == 1, 'price'] += 0.10\n\n# New market shares after the price increase\nnew_market_shares = predict_market_shares(result.x, long_data_cf)\n\n# Compare the market shares\nprint('Original market shares:')\nprint(original_market_shares)\nprint('New market shares after price increase:')\nprint(new_market_shares)\n\nDollar benefit between most and least preferred yogurts: -0.12072647932202944\nOriginal market shares:\nproduct\n1    0.341975\n2    0.401235\n3    0.029218\n4    0.227572\nName: probabilities, dtype: float64\nNew market shares after price increase:\nproduct\n1    0.021118\n2    0.591145\n3    0.044040\n4    0.343697\nName: probabilities, dtype: float64\n\n\nDollar benefit between most and least preferred yogurts: -0.12072647932202944\nNew market shares after price increase: product 1 0.021118 2 0.591145 3 0.044040 4 0.343697\nAfter making the analysis we conclude that the market share value for yogurt one decrease by more than .30."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/project4/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\ntodo: download the dataset from here: http://goo.gl/5xQObB\n\ndata = pd.read_csv('rintro-chapter13conjoint.csv')\n\n# Display the first few rows of the dataset\ndata.head()\n\nnum_respondents = data['resp.id'].nunique()\nchoice_tasks_per_respondent = data.groupby('resp.id')['ques'].nunique().mean()\nalternatives_per_task = data.groupby(['resp.id', 'ques']).size().mean()\n\n# Attributes and levels\nseat_levels = data['seat'].unique()\ncargo_levels = data['cargo'].unique()\nengine_levels = data['eng'].unique()\nprice_levels = data['price'].unique()\n\nnum_respondents, choice_tasks_per_respondent, alternatives_per_task, seat_levels, cargo_levels, engine_levels, price_levels\n\n(200,\n 15.0,\n 3.0,\n array([6, 8, 7]),\n array(['2ft', '3ft'], dtype=object),\n array(['gas', 'hyb', 'elec'], dtype=object),\n array([35, 30, 40]))\n\n\nSurvey Participants:\nNumber of respondents: 200\nSurvey Structure:\nChoice tasks per respondent: Each respondent completed an average of 15 choice tasks.\nAlternatives per choice task: Each choice task presented 3 alternatives.\nAttributes and Levels:\nThe survey evaluated the following attributes for each car alternative:\nNumber of seats: 6, 7, 8\nCargo space: 2ft, 3ft\nEngine type: Gas, Hybrid (hyb), Electric (elec)\nPrice: 30, 35, 40 (in thousands of dollars)\n\n\nModel\n\nimport statsmodels.api as sm\nimport numpy as np\n\ndata['choice'] = data['choice'].astype(int)\ndata['price'] = data['price'].astype(float)\n\n# Convert to categorical and specify reference categories\ndata['seat'] = data['seat'].astype('category').cat.reorder_categories([6, 7, 8], ordered=True)\ndata['cargo'] = data['cargo'].astype('category').cat.reorder_categories(['2ft', '3ft'], ordered=True)\ndata['eng'] = data['eng'].astype('category').cat.reorder_categories(['gas', 'hyb', 'elec'], ordered=True)\n\n# Create dummy variables with the correct reference categories\ndata_dummies = pd.get_dummies(data, drop_first=True)\n\n# Define the dependent variable and independent variables\ny = data['choice']\nX = data_dummies[['seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec', 'price']]\n\n# Convert X to a numeric type\nX = sm.add_constant(X.astype(float))\n\n# Fit the MNL model using statsmodels\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\n# Display the coefficients and standard errors\ncoefficients = result.params\nstandard_errors = result.bse\n\n# Print results\nprint(\"Coefficients:\\n\", coefficients)\nprint(\"\\nStandard Errors:\\n\", standard_errors)\n\nOptimization terminated successfully.\n         Current function value: 0.558663\n         Iterations 6\nCoefficients:\n const        5.532174\nseat_7      -0.524752\nseat_8      -0.293085\ncargo_3ft    0.438538\neng_hyb     -0.760489\neng_elec    -1.434680\nprice       -0.159133\ndtype: float64\n\nStandard Errors:\n const        0.224186\nseat_7       0.059634\nseat_8       0.058510\ncargo_3ft    0.048706\neng_hyb      0.056919\neng_elec     0.061794\nprice        0.006212\ndtype: float64\n\n\n\n\nResults\nCoefficients:\nConstant: 5.532174\nSeats (7): -0.524752\nSeats (8): -0.293085\nCargo (3ft): 0.438538\nEngine (Hybrid): -0.760489\nEngine (Electric): -1.434680\nPrice: -0.159133\nStandard Errors:\nConstant: 0.224186\nSeats (7): 0.059634\nSeats (8): 0.058510\nCargo (3ft): 0.048706\nEngine (Hybrid): 0.056919\nEngine (Electric): 0.061794\nPrice: 0.006212\nPreferred Features: More Cargo Space: 3ft cargo space is preferred over 2ft. 6 Seats: Preferred over 7 or 8 seats. Gas Engines: Strongly preferred over hybrid and electric engines. Lower Price: Lower prices increase the likelihood of choice.\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n# Create a dataframe for the six minivans\nminivans = pd.DataFrame({\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat': [7, 6, 8, 7, 6, 7],\n    'cargo': ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    'eng': ['hyb', 'gas', 'gas', 'gas', 'elec', 'hyb'],\n    'price': [30, 30, 30, 40, 40, 35]\n})\n\n# Create dummy variables for the minivans and ensure all expected columns are present\nminivans_dummies = pd.get_dummies(minivans, columns=['seat', 'cargo', 'eng'])\n\n# Ensure all required columns are present by adding missing columns with zero values\nrequired_columns = ['seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'eng_elec', 'price']\nfor col in required_columns:\n    if col not in minivans_dummies.columns:\n        minivans_dummies[col] = 0\n\n# Select the relevant columns in the correct order\nX_minivans = minivans_dummies[required_columns]\n\n# Add a constant to the independent variables\nX_minivans = sm.add_constant(X_minivans.astype(float))\n\n# Predict the utility of each minivan\nutilities = np.dot(X_minivans, coefficients)\n\n# Calculate the exponential of utilities to get the market shares\nexp_utilities = np.exp(utilities)\nmarket_shares = exp_utilities / np.sum(exp_utilities)\n\n# Combine the results into a dataframe\nminivans['Market_Share'] = market_shares\n\nprint(minivans[['Minivan', 'Market_Share']])\n\n  Minivan  Market_Share\n0       A      0.116080\n1       B      0.419692\n2       C      0.313073\n3       D      0.078412\n4       E      0.020359\n5       F      0.052385\n\n\nInterpretation\nMinivan B (6 seats, 2ft cargo, gas engine, $30k) has the highest predicted market share at 41.97%.\nMinivan C (8 seats, 2ft cargo, gas engine, $30k) follows with a market share of 31.31%.\nMinivan A (7 seats, 2ft cargo, hybrid engine, $30k) has a market share of 11.61%.\nMinivan D (7 seats, 3ft cargo, gas engine, $40k) has a market share of 7.84%.\nMinivan F (7 seats, 2ft cargo, hybrid engine, $35k) has a market share of 5.24%.\nMinivan E (6 seats, 2ft cargo, electric engine, $40k) has the lowest market share at 2.04%.\nThese results indicate that:\nLower price and the presence of a gas engine are highly preferred, as seen with Minivans B and C.\nLarger cargo space (3ft) does not compensate for a higher price or a less preferred engine type (e.g., Minivan D).\nHybrid and electric engines are less preferred compared to gas engines, especially at higher prices."
  },
  {
    "objectID": "projects/project5/hw4_questions.html",
    "href": "projects/project5/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost."
  },
  {
    "objectID": "projects/project5/hw4_questions.html#introduction",
    "href": "projects/project5/hw4_questions.html#introduction",
    "title": "Key Drivers Analysis",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nDataset Description\nThe dataset utilized in this analysis contains survey responses from customers about their experiences and satisfaction with various payment cards. The survey covers multiple features of the cards, such as trustworthiness, ease of use, rewards, and customer service. Each response is linked to several variables representing these features, along with an overall satisfaction score. Objective\nThe main goal of this article is to examine and compare different methods for calculating the importance of various features (variables) in determining customer satisfaction. Identifying which features most significantly impact satisfaction can provide valuable insights for businesses aiming to enhance their payment card products and services. The methods investigated in this analysis include Pearson correlations, standardized regression coefficients, Shapley values, permutation importance, Johnson’s relative weights, mean decrease in Gini coefficient, and XGBoost feature importance\n\n\nFeatures\nbrand: Identifier for the brand of the payment card. id: Unique identifier for each survey response. satisfaction: Overall satisfaction score given by the customer (target variable). trust: Indicates the trustworthiness of the payment card as perceived by the customer. build: Reflects the physical build quality of the card. differs: Indicates how different the card is compared to others in the market. easy: Measures the ease of use of the card. appealing: Represents the visual and aesthetic appeal of the card. rewarding: Reflects the rewards and benefits associated with the card. popular: Indicates the perceived popularity of the card. service: Measures the quality of customer service related to the card. impact: Reflects the overall impact of the card on the customer’s life."
  },
  {
    "objectID": "projects/project5/hw4_questions.html#key-driver-methods",
    "href": "projects/project5/hw4_questions.html#key-driver-methods",
    "title": "Key Drivers Analysis",
    "section": "2. Key Driver Methods",
    "text": "2. Key Driver Methods\n\nPearson Correlation\nPearson correlation measures the linear relationship between two variables. We will calculate the correlation between each feature and the overall satisfaction score.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nimport shap\nimport statsmodels.api as sm\nfrom scipy.stats import pearsonr\n\n# Load the dataset\nfile_path = 'data_for_drivers_analysis.csv'\ndata = pd.read_csv(file_path)\n\n# Extract features and target variable\nX = data.drop(columns=['brand', 'id', 'satisfaction'])\ny = data['satisfaction']\n\n# Standardize the features and target variable\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ny_scaled = scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n\n# Initialize the results dictionary\nresults = {}\n\n# Step 1: Pearson Correlations\npearson_correlations = {feature: pearsonr(X[feature], y)[0] for feature in X.columns}\nresults['Pearson Correlations'] = pearson_correlations\n\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#standardized-regression-coefficients",
    "href": "projects/project5/hw4_questions.html#standardized-regression-coefficients",
    "title": "Key Drivers Analysis",
    "section": "Standardized Regression coefficients",
    "text": "Standardized Regression coefficients\nStandardized regression coefficients (beta weights) indicate the relative importance of each predictor in a regression model. We will standardize the features and target variable and then fit a linear regression model to get these coefficients.\n\nregression = LinearRegression()\nregression.fit(X_scaled, y_scaled)\nstandardized_coefficients = dict(zip(X.columns, regression.coef_))\nresults['Standardized Regression Coefficients'] = standardized_coefficients"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#usefulness",
    "href": "projects/project5/hw4_questions.html#usefulness",
    "title": "Key Drivers Analysis",
    "section": "Usefulness",
    "text": "Usefulness\nIn this context, “usefulness” refers to the contribution of each feature to the explanatory power of the model. We will calculate it using a custom approach, which often involves comparing the R-squared values of models with and without the feature.\n\ndef usefulness_score(model, X, y):\n    r_squared_full = model.score(X, y)\n    usefulness_scores = {}\n    for feature in X.columns:\n        X_reduced = X.drop(columns=[feature])\n        model.fit(X_reduced, y)\n        r_squared_reduced = model.score(X_reduced, y)\n        usefulness_scores[feature] = r_squared_full - r_squared_reduced\n    return usefulness_scores\n\nusefulness = usefulness_score(regression, pd.DataFrame(X_scaled, columns=X.columns), y_scaled)\nresults['Usefulness'] = usefulness\n\nX has feature names, but LinearRegression was fitted without feature names"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#permutation-importance",
    "href": "projects/project5/hw4_questions.html#permutation-importance",
    "title": "Key Drivers Analysis",
    "section": "Permutation Importance",
    "text": "Permutation Importance\n\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Refit the linear regression model to ensure it aligns with the feature set\nregression.fit(X_scaled_df, y_scaled)\n\n# Calculate permutation importance for the linear regression model\npermutation_importances = permutation_importance(regression, X_scaled_df, y_scaled, n_repeats=30, random_state=42)\nperm_importances_dict = dict(zip(X.columns, permutation_importances.importances_mean))\nresults['Permutation Importance'] = perm_importances_dict"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#johnsons-relative-weights",
    "href": "projects/project5/hw4_questions.html#johnsons-relative-weights",
    "title": "Key Drivers Analysis",
    "section": "Johnson’s relative weights",
    "text": "Johnson’s relative weights\nJohnson’s relative weights decompose the regression R-squared into components attributable to each predictor, considering multicollinearity. We will calculate these weights to understand each feature’s relative importance.\n\ndef relative_weights(X, y):\n    model = sm.OLS(y, sm.add_constant(X)).fit()\n    R = np.corrcoef(X, rowvar=False)\n    Z = X.dot(np.linalg.inv(np.sqrt(R)))\n    relative_weights = np.square(np.corrcoef(Z, y, rowvar=False)[-1, :-1])\n    return dict(zip(X.columns, relative_weights / relative_weights.sum()))\n\nrelative_weights = relative_weights(pd.DataFrame(X_scaled, columns=X.columns), y_scaled)\nresults['Johnson\\'s Relative Weights'] = relative_weights"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#gini-coefficient",
    "href": "projects/project5/hw4_questions.html#gini-coefficient",
    "title": "Key Drivers Analysis",
    "section": "Gini coefficient",
    "text": "Gini coefficient\nThe mean decrease in Gini coefficient is used in random forests to measure the importance of each feature. It reflects how much including a particular feature decreases the impurity of the model.\n\nforest = RandomForestRegressor(random_state=42)\nforest.fit(X, y)\nmdi_importances = forest.feature_importances_\nmdi_importances_dict = dict(zip(X.columns, mdi_importances))\nresults['Mean Decrease in Gini Coefficient'] = mdi_importances_dict"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#results",
    "href": "projects/project5/hw4_questions.html#results",
    "title": "Key Drivers Analysis",
    "section": "Results",
    "text": "Results\n\nresults_df = pd.DataFrame(results)\n\nresults_df\n\n\n\n\n\n\n\n\nPearson Correlations\nStandardized Regression Coefficients\nUsefulness\nPermutation Importance\nJohnson's Relative Weights\nMean Decrease in Gini Coefficient\n\n\n\n\ntrust\n0.255706\n0.115752\n0.008243\n0.027039\n0.317137\n0.155865\n\n\nbuild\n0.191896\n0.019979\n0.000266\n0.000866\n0.005107\n0.102301\n\n\ndiffers\n0.184801\n0.027847\n0.000550\n0.001641\n0.009521\n0.089897\n\n\neasy\n0.212985\n0.021970\n0.000289\n0.001097\n0.000002\n0.099904\n\n\nappealing\n0.207997\n0.033835\n0.000710\n0.002386\n0.005827\n0.085534\n\n\nrewarding\n0.194561\n0.005067\n0.000016\n0.000097\n0.017286\n0.101057\n\n\npopular\n0.171425\n0.016616\n0.000204\n0.000523\n0.038779\n0.094944\n\n\nservice\n0.251098\n0.088390\n0.004674\n0.016162\n0.172526\n0.129664\n\n\nimpact\n0.254539\n0.128422\n0.011203\n0.033854\n0.433816\n0.140834"
  },
  {
    "objectID": "projects/project5/hw4_questions.html#results-interpretation",
    "href": "projects/project5/hw4_questions.html#results-interpretation",
    "title": "Key Drivers Analysis",
    "section": "3. Results interpretation",
    "text": "3. Results interpretation\n\nPearson Correlations\n\nPearson correlation measures the linear relationship between each feature and customer satisfaction. Higher absolute values indicate a stronger linear relationship.\nTop Features: trust (0.256), impact (0.255), and service (0.251)\n\nStandardized Regression Coefficients\n\nThese coefficients show the relative importance of each predictor in a standardized linear regression model.\nTop Features: impact (0.128), trust (0.116), and service (0.088)\n\nUsefulness\n\n“Usefulness” measures the decrease in the model’s R-squared when a feature is removed, indicating its contribution to the model’s explanatory power.\nTop Features: impact (0.011), trust (0.008), and service (0.005)\n\nPermutation Importance\n\nThis method evaluates the decrease in model performance when the values of a single feature are randomly shuffled.\nTop Features: impact (0.034), trust (0.027), and service (0.016)\n\nJohnson’s Relative Weights\n\nJohnson’s relative weights decompose the regression R-squared into components attributable to each predictor, considering multicollinearity.\nTop Features: impact (0.434), trust (0.317), and service (0.173)\n\nMean Decrease in Gini Coefficient from Random Forest\n\nThe mean decrease in Gini coefficient measures how much including a particular feature decreases the impurity of the model.\nTop Features: trust (0.156), impact (0.141), and service (0.130)\n\nOverall Insights\n\nConsistently Important Features: trust, impact, and service are consistently ranked as important across all metrics, indicating their strong influence on customer satisfaction.\nModerately Important Features: easy, appealing, and rewarding show moderate importance in some metrics but not in others.\nLess Important Features: build, differs, and popular generally show lower importance scores across the different metrics.\n\nThese insights can help businesses prioritize which features to focus on to enhance customer satisfaction with their payment cards."
  },
  {
    "objectID": "projects/project6/hw5_questions.html",
    "href": "projects/project6/hw5_questions.html",
    "title": "Segmentation Methods",
    "section": "",
    "text": "Initialization: Randomly select kk initial centroids.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris_data = pd.read_csv('iris.csv')\n\ndef initialize_centroids(data, k):\n    \"\"\"Randomly initialize k centroids from the data.\"\"\"\n    n_samples, n_features = data.shape\n    centroids = data[np.random.choice(n_samples, k, replace=False)]\n    return centroids\n\n# Extracting features from the dataset\nX = iris_data.iloc[:, :-1].values\n\n# Initialize centroids for k=3\nk = 3\ninitial_centroids = initialize_centroids(X, k)\n\n# Plot initial centroids\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Data Points')\nplt.scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', marker='x', s=100, label='Initial Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Initial Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAssignment Step: Assign each data point to the nearest centroid.\n\n\ndef assign_clusters(data, centroids):\n    \"\"\"Assign each data point to the nearest centroid.\"\"\"\n    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n    cluster_labels = np.argmin(distances, axis=1)\n    return cluster_labels\n\n# Assign clusters based on initial centroids\ncluster_labels = assign_clusters(X, initial_centroids)\n\n# Plot the assigned clusters\nplt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', marker='x', s=100, label='Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Cluster Assignment')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUpdate Step: Calculate the new centroids as the mean of the points assigned to each centroid.\n\n\ndef update_centroids(data, cluster_labels, k):\n    \"\"\"Calculate the new centroids as the mean of the points assigned to each centroid.\"\"\"\n    new_centroids = np.array([data[cluster_labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\n# Update centroids based on the initial cluster assignment\nnew_centroids = update_centroids(X, cluster_labels, k)\n\n# Plot the updated centroids\nplt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='red', marker='x', s=100, label='New Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Updated Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRepeat: Repeat the assignment and update steps until the centroids do not change significantly.\n\n\ndef k_means(data, k, max_iters=100, tol=1e-4):\n    \"\"\"K-means clustering algorithm.\"\"\"\n    centroids = initialize_centroids(data, k)\n    for i in range(max_iters):\n        cluster_labels = assign_clusters(data, centroids)\n        new_centroids = update_centroids(data, cluster_labels, k)\n        \n        # Check for convergence\n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n        centroids = new_centroids\n    \n    return centroids, cluster_labels\n\n# Run the k-means algorithm\nfinal_centroids, final_cluster_labels = k_means(X, k)\n\n# Plot the final clusters\nplt.scatter(X[:, 0], X[:, 1], c=final_cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(final_centroids[:, 0], final_centroids[:, 1], c='red', marker='x', s=100, label='Final Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Final Clusters and Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Calculate within-cluster sum of squares and silhouette scores for k=2 to k=7\nwcss = []\nsil_scores = []\n\nfor k in range(2, 8):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    \n    wcss.append(kmeans.inertia_)\n    sil_scores.append(silhouette_score(X, kmeans.labels_))\n    \n    # Plot clusters\n    plt.figure()\n    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', label='Data Points')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100, label='Centroids')\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.legend()\n    plt.title(f'Clusters and Centroids (k={k})')\n    plt.show()\n\n# Plot WCSS and Silhouette Scores\nplt.figure()\nplt.plot(range(2, 8), wcss, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('WCSS vs. Number of Clusters')\nplt.show()\n\nplt.figure()\nplt.plot(range(2, 8), sil_scores, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters')\nplt.show()\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run the custom k-means algorithm for k=3\ncustom_centroids, custom_labels = k_means(X, k=3)\n\n# Print custom centroids and labels\nprint(\"Custom K-means Centroids:\\n\", custom_centroids)\nprint(\"Custom K-means Labels:\\n\", custom_labels)\n\nCustom K-means Centroids:\n [[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\nCustom K-means Labels:\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n\n\n\n# Run the scikit-learn KMeans algorithm for k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\nsklearn_centroids = kmeans.cluster_centers_\nsklearn_labels = kmeans.labels_\n\n# Print scikit-learn centroids and labels\nprint(\"Scikit-learn KMeans Centroids:\\n\", sklearn_centroids)\nprint(\"Scikit-learn KMeans Labels:\\n\", sklearn_labels)\n\nfrom scipy.spatial.distance import cdist\n\n# Find the closest custom centroid to each sklearn centroid\ncentroid_order = cdist(sklearn_centroids, custom_centroids).argmin(axis=1)\n\n# Reorder custom labels to match sklearn labels\naligned_custom_labels = np.zeros_like(custom_labels)\nfor i in range(3):\n    aligned_custom_labels[custom_labels == i] = centroid_order[i]\n\n# Compare the labels\nalignment_accuracy = np.mean(aligned_custom_labels == sklearn_labels)\nalignment_accuracy\n\n# Plot clusters from custom k-means implementation\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], c=aligned_custom_labels, cmap='viridis', label='Data Points')\nplt.scatter(custom_centroids[:, 0], custom_centroids[:, 1], c='red', marker='x', s=100, label='Custom Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Custom K-means Clusters')\n\n# Plot clusters from scikit-learn k-means implementation\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 0], X[:, 1], c=sklearn_labels, cmap='viridis', label='Data Points')\nplt.scatter(sklearn_centroids[:, 0], sklearn_centroids[:, 1], c='red', marker='x', s=100, label='Sklearn Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Scikit-learn KMeans Clusters')\n\nplt.tight_layout()\nplt.show()\n\nScikit-learn KMeans Centroids:\n [[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\nScikit-learn KMeans Labels:\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the centroids are the same but in a different order. This is expected because k-means does not guarantee the same order of centroids.\nThe visual representation is pretty close on the selection of the centroids and the groupings per cluster\nNow we wil test the WCSS to compare the models.\n\n# Function to calculate WCSS and silhouette scores for custom k-means\ndef calculate_metrics_custom(data, max_k):\n    wcss_custom = []\n    sil_scores_custom = []\n    \n    for k in range(2, max_k + 1):\n        centroids, labels = k_means(data, k)\n        wcss = np.sum(np.min(cdist(data, centroids), axis=1)**2)\n        sil_score = silhouette_score(data, labels)\n        \n        wcss_custom.append(wcss)\n        sil_scores_custom.append(sil_score)\n    \n    return wcss_custom, sil_scores_custom\n\n# Calculate metrics for custom k-means\nmax_k = 7\nwcss_custom, sil_scores_custom = calculate_metrics_custom(X, max_k)\n\n# Function to calculate WCSS and silhouette scores for scikit-learn k-means\ndef calculate_metrics_sklearn(data, max_k):\n    wcss_sklearn = []\n    sil_scores_sklearn = []\n    \n    for k in range(2, max_k + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(data)\n        \n        wcss_sklearn.append(kmeans.inertia_)\n        sil_scores_sklearn.append(silhouette_score(data, kmeans.labels_))\n    \n    return wcss_sklearn, sil_scores_sklearn\n\n# Calculate metrics for scikit-learn k-means\nwcss_sklearn, sil_scores_sklearn = calculate_metrics_sklearn(X, max_k)\n\n# Plot WCSS for both custom and scikit-learn k-means\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(2, max_k + 1), wcss_custom, marker='o', label='Custom K-means')\nplt.plot(range(2, max_k + 1), wcss_sklearn, marker='o', label='Scikit-learn KMeans')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('WCSS vs. Number of Clusters')\nplt.legend()\n\n# Plot Silhouette Scores for both custom and scikit-learn k-means\nplt.subplot(1, 2, 2)\nplt.plot(range(2, max_k + 1), sil_scores_custom, marker='o', label='Custom K-means')\nplt.plot(range(2, max_k + 1), sil_scores_sklearn, marker='o', label='Scikit-learn KMeans')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\nWithin-Cluster Sum of Squares (WCSS) Both implementations show a similar decreasing trend in WCSS as the number of clusters increases. The “elbow” point, which indicates a significant reduction in WCSS, appears around k=3k=3, suggesting that 3 clusters might be optimal.\nSilhouette Scores Both implementations show similar patterns in silhouette scores. The highest silhouette scores are observed for k=2k=2 and k=3k=3, with k=3k=3 being slightly higher in both cases, indicating that 3 clusters provide well-defined separation.\nConclusion\nBoth the custom k-means implementation and the scikit-learn KMeans implementation suggest that 3 clusters are optimal for the Iris dataset. The comparison shows that both methods produce consistent results, confirming that the custom implementation is working correctly."
  },
  {
    "objectID": "projects/project6/hw5_questions.html#k-means",
    "href": "projects/project6/hw5_questions.html#k-means",
    "title": "Segmentation Methods",
    "section": "",
    "text": "Initialization: Randomly select kk initial centroids.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris_data = pd.read_csv('iris.csv')\n\ndef initialize_centroids(data, k):\n    \"\"\"Randomly initialize k centroids from the data.\"\"\"\n    n_samples, n_features = data.shape\n    centroids = data[np.random.choice(n_samples, k, replace=False)]\n    return centroids\n\n# Extracting features from the dataset\nX = iris_data.iloc[:, :-1].values\n\n# Initialize centroids for k=3\nk = 3\ninitial_centroids = initialize_centroids(X, k)\n\n# Plot initial centroids\nplt.scatter(X[:, 0], X[:, 1], c='blue', label='Data Points')\nplt.scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', marker='x', s=100, label='Initial Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Initial Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAssignment Step: Assign each data point to the nearest centroid.\n\n\ndef assign_clusters(data, centroids):\n    \"\"\"Assign each data point to the nearest centroid.\"\"\"\n    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n    cluster_labels = np.argmin(distances, axis=1)\n    return cluster_labels\n\n# Assign clusters based on initial centroids\ncluster_labels = assign_clusters(X, initial_centroids)\n\n# Plot the assigned clusters\nplt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', marker='x', s=100, label='Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Cluster Assignment')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUpdate Step: Calculate the new centroids as the mean of the points assigned to each centroid.\n\n\ndef update_centroids(data, cluster_labels, k):\n    \"\"\"Calculate the new centroids as the mean of the points assigned to each centroid.\"\"\"\n    new_centroids = np.array([data[cluster_labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\n# Update centroids based on the initial cluster assignment\nnew_centroids = update_centroids(X, cluster_labels, k)\n\n# Plot the updated centroids\nplt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='red', marker='x', s=100, label='New Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Updated Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRepeat: Repeat the assignment and update steps until the centroids do not change significantly.\n\n\ndef k_means(data, k, max_iters=100, tol=1e-4):\n    \"\"\"K-means clustering algorithm.\"\"\"\n    centroids = initialize_centroids(data, k)\n    for i in range(max_iters):\n        cluster_labels = assign_clusters(data, centroids)\n        new_centroids = update_centroids(data, cluster_labels, k)\n        \n        # Check for convergence\n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n        centroids = new_centroids\n    \n    return centroids, cluster_labels\n\n# Run the k-means algorithm\nfinal_centroids, final_cluster_labels = k_means(X, k)\n\n# Plot the final clusters\nplt.scatter(X[:, 0], X[:, 1], c=final_cluster_labels, cmap='viridis', label='Data Points')\nplt.scatter(final_centroids[:, 0], final_centroids[:, 1], c='red', marker='x', s=100, label='Final Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Final Clusters and Centroids')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Calculate within-cluster sum of squares and silhouette scores for k=2 to k=7\nwcss = []\nsil_scores = []\n\nfor k in range(2, 8):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    \n    wcss.append(kmeans.inertia_)\n    sil_scores.append(silhouette_score(X, kmeans.labels_))\n    \n    # Plot clusters\n    plt.figure()\n    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', label='Data Points')\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100, label='Centroids')\n    plt.xlabel('Sepal Length')\n    plt.ylabel('Sepal Width')\n    plt.legend()\n    plt.title(f'Clusters and Centroids (k={k})')\n    plt.show()\n\n# Plot WCSS and Silhouette Scores\nplt.figure()\nplt.plot(range(2, 8), wcss, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('WCSS vs. Number of Clusters')\nplt.show()\n\nplt.figure()\nplt.plot(range(2, 8), sil_scores, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters')\nplt.show()\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run the custom k-means algorithm for k=3\ncustom_centroids, custom_labels = k_means(X, k=3)\n\n# Print custom centroids and labels\nprint(\"Custom K-means Centroids:\\n\", custom_centroids)\nprint(\"Custom K-means Labels:\\n\", custom_labels)\n\nCustom K-means Centroids:\n [[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\nCustom K-means Labels:\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n\n\n\n# Run the scikit-learn KMeans algorithm for k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\nsklearn_centroids = kmeans.cluster_centers_\nsklearn_labels = kmeans.labels_\n\n# Print scikit-learn centroids and labels\nprint(\"Scikit-learn KMeans Centroids:\\n\", sklearn_centroids)\nprint(\"Scikit-learn KMeans Labels:\\n\", sklearn_labels)\n\nfrom scipy.spatial.distance import cdist\n\n# Find the closest custom centroid to each sklearn centroid\ncentroid_order = cdist(sklearn_centroids, custom_centroids).argmin(axis=1)\n\n# Reorder custom labels to match sklearn labels\naligned_custom_labels = np.zeros_like(custom_labels)\nfor i in range(3):\n    aligned_custom_labels[custom_labels == i] = centroid_order[i]\n\n# Compare the labels\nalignment_accuracy = np.mean(aligned_custom_labels == sklearn_labels)\nalignment_accuracy\n\n# Plot clusters from custom k-means implementation\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], c=aligned_custom_labels, cmap='viridis', label='Data Points')\nplt.scatter(custom_centroids[:, 0], custom_centroids[:, 1], c='red', marker='x', s=100, label='Custom Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Custom K-means Clusters')\n\n# Plot clusters from scikit-learn k-means implementation\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 0], X[:, 1], c=sklearn_labels, cmap='viridis', label='Data Points')\nplt.scatter(sklearn_centroids[:, 0], sklearn_centroids[:, 1], c='red', marker='x', s=100, label='Sklearn Centroids')\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.legend()\nplt.title('Scikit-learn KMeans Clusters')\n\nplt.tight_layout()\nplt.show()\n\nScikit-learn KMeans Centroids:\n [[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\nScikit-learn KMeans Labels:\n [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the centroids are the same but in a different order. This is expected because k-means does not guarantee the same order of centroids.\nThe visual representation is pretty close on the selection of the centroids and the groupings per cluster\nNow we wil test the WCSS to compare the models.\n\n# Function to calculate WCSS and silhouette scores for custom k-means\ndef calculate_metrics_custom(data, max_k):\n    wcss_custom = []\n    sil_scores_custom = []\n    \n    for k in range(2, max_k + 1):\n        centroids, labels = k_means(data, k)\n        wcss = np.sum(np.min(cdist(data, centroids), axis=1)**2)\n        sil_score = silhouette_score(data, labels)\n        \n        wcss_custom.append(wcss)\n        sil_scores_custom.append(sil_score)\n    \n    return wcss_custom, sil_scores_custom\n\n# Calculate metrics for custom k-means\nmax_k = 7\nwcss_custom, sil_scores_custom = calculate_metrics_custom(X, max_k)\n\n# Function to calculate WCSS and silhouette scores for scikit-learn k-means\ndef calculate_metrics_sklearn(data, max_k):\n    wcss_sklearn = []\n    sil_scores_sklearn = []\n    \n    for k in range(2, max_k + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(data)\n        \n        wcss_sklearn.append(kmeans.inertia_)\n        sil_scores_sklearn.append(silhouette_score(data, kmeans.labels_))\n    \n    return wcss_sklearn, sil_scores_sklearn\n\n# Calculate metrics for scikit-learn k-means\nwcss_sklearn, sil_scores_sklearn = calculate_metrics_sklearn(X, max_k)\n\n# Plot WCSS for both custom and scikit-learn k-means\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(2, max_k + 1), wcss_custom, marker='o', label='Custom K-means')\nplt.plot(range(2, max_k + 1), wcss_sklearn, marker='o', label='Scikit-learn KMeans')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('WCSS vs. Number of Clusters')\nplt.legend()\n\n# Plot Silhouette Scores for both custom and scikit-learn k-means\nplt.subplot(1, 2, 2)\nplt.plot(range(2, max_k + 1), sil_scores_custom, marker='o', label='Custom K-means')\nplt.plot(range(2, max_k + 1), sil_scores_sklearn, marker='o', label='Scikit-learn KMeans')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/opt/conda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\n\nWithin-Cluster Sum of Squares (WCSS) Both implementations show a similar decreasing trend in WCSS as the number of clusters increases. The “elbow” point, which indicates a significant reduction in WCSS, appears around k=3k=3, suggesting that 3 clusters might be optimal.\nSilhouette Scores Both implementations show similar patterns in silhouette scores. The highest silhouette scores are observed for k=2k=2 and k=3k=3, with k=3k=3 being slightly higher in both cases, indicating that 3 clusters provide well-defined separation.\nConclusion\nBoth the custom k-means implementation and the scikit-learn KMeans implementation suggest that 3 clusters are optimal for the Iris dataset. The comparison shows that both methods produce consistent results, confirming that the custom implementation is working correctly."
  }
]