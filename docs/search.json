[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Your Name",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nBidkar Hinojosa Leal\n\n\nApr 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\nfile_path = 'karlan_list_2007.dta'\ndf = pd.read_stata(file_path)\nprint(df)\n\npd.set_option('display.max_columns', None)\ndf.describe(include='all')\n\n#66% of the data is treatment and 33% is control\n#The mean amount is less than 1 \n#The number of prior donations is 8\n#Only 27% of the population is female\n#pblack and hhmedianincome have the most NaN values in the dataset\n\nnan_counts = df.isna().sum()\nnan_counts\n\n       treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0              0        1  Control       0       0   Control       0       0   \n1              0        1  Control       0       0   Control       0       0   \n2              1        0        1       0       0  $100,000       0       0   \n3              1        0        1       0       0  Unstated       0       0   \n4              1        0        1       0       0   $50,000       0       1   \n...          ...      ...      ...     ...     ...       ...     ...     ...   \n50078          1        0        1       0       0   $25,000       1       0   \n50079          0        1  Control       0       0   Control       0       0   \n50080          0        1  Control       0       0   Control       0       0   \n50081          1        0        3       0       1  Unstated       0       0   \n50082          1        0        3       0       1   $25,000       1       0   \n\n       size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0            0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1            0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2            1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3            0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4            0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n...        ...     ...  ...    ...      ...       ...       ...        ...   \n50078        0       0  ...    0.0      1.0  0.872797  0.089959   0.257265   \n50079        0       0  ...    0.0      1.0  0.688262  0.108889   0.288792   \n50080        0       0  ...    1.0      0.0  0.900000  0.021311   0.178689   \n50081        0       1  ...    1.0      0.0  0.917206  0.008257   0.225619   \n50082        0       0  ...    0.0      1.0  0.530023  0.074112   0.340698   \n\n       ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0           2.10          28517.0  0.499807      0.324528       1.000000  \n1            NaN              NaN       NaN           NaN            NaN  \n2           2.48          51175.0  0.721941      0.192668       1.000000  \n3           2.65          79269.0  0.920431      0.412142       1.000000  \n4           1.85          40908.0  0.416072      0.439965       1.000000  \n...          ...              ...       ...           ...            ...  \n50078       2.13          45047.0  0.771316      0.263744       1.000000  \n50079       2.67          74655.0  0.741931      0.586466       1.000000  \n50080       2.36          26667.0  0.778689      0.107930       0.000000  \n50081       2.57          39530.0  0.733988      0.184768       0.634903  \n50082       3.70          48744.0  0.717843      0.127941       0.994181  \n\n[50083 rows x 51 columns]\n\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\ncontrol_group = df[df['treatment'] == 0]['mrm2']\ntreatment_group = df[df['treatment'] == 1]['mrm2']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['mrm2'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['mrm2']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n# I get the same p value for the treatment which is .905 which means we cannot say with 95% confidence that treatment and control have different number of months since last donation\n\ncontrol_group = df[df['treatment'] == 0]['amount']\ntreatment_group = df[df['treatment'] == 1]['amount']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['amount'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['amount']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n#The p value for .063 make us not reject the null hypothesis so we cant say the amount donated changes from control to treatment.\n\ncontrol_group = df[df['treatment'] == 0]['hpa']\ntreatment_group = df[df['treatment'] == 1]['hpa']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ndf= df.dropna(subset=['amount'])\ndf['intercept'] = 1\n\nX = df[['intercept', 'treatment']]\n\ny = df['hpa']\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n#The highest previous contribution does not change from control to treatment, null hypothesis cannot be rejected.\n\nT-test results -- t-statistic: -0.1194921058159193 p-value: 0.9048859731777738\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.905\nTime:                        18:03:49   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test results -- t-statistic: -1.8608252329783106 p-value: 0.06277471702258718\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.464\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0627\nTime:                        18:03:49   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50082   AIC:                         3.589e+05\nDf Residuals:                   50080   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96858.715   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240721274.363\nSkew:                          15.296   Prob(JB):                         0.00\nKurtosis:                     341.262   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nT-test results -- t-statistic: -0.9467832370069835 p-value: 0.34375381350882095\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    hpa   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.8949\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.344\nTime:                        18:03:49   Log-Likelihood:            -2.8467e+05\nNo. Observations:               50082   AIC:                         5.693e+05\nDf Residuals:                   50080   BIC:                         5.694e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     58.9602      0.551    107.005      0.000      57.880      60.040\ntreatment      0.6389      0.675      0.947      0.344      -0.684       1.961\n==============================================================================\nOmnibus:                    66198.015   Durbin-Watson:                   2.003\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         14447952.958\nSkew:                           7.552   Prob(JB):                         0.00\nKurtosis:                      84.826   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf['group'] = df['control'].map({0: 'treatment', 1: 'control'})\nproportions = df.groupby('group')['gave'].mean().reset_index()\nproportions.rename(columns={'gave': 'proportion'}, inplace=True)\n\nsns.barplot(x='group', y='proportion', data=proportions)\nplt.title('Proportion of Donations in Treatment and Control Groups')\nplt.ylabel('Proportion of People Who Donated')\nplt.xlabel('Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\n\ncontrol_group = df[df['treatment'] == 0]['gave']\ntreatment_group = df[df['treatment'] == 1]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\\\n\ndf['intercept'] = 1\n\n# Define the model\nmodel = sm.OLS(df['gave'], df[['intercept', 'control']])\nresults = model.fit()\n\nprint(results.summary())\n\n#The results give a p value under .05 which means we can reject the null hypothesis. In a simpler wording, we can conclude that the xcontrol and treatment groups have a significant difference in the percentage of people who donated, meaning there was a higher percentage of people in the treatment group which can be observed in the graph.\n\nT-test results -- t-statistic: -3.101804902209603 p-value: 0.0019245154824258396\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.621\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):            0.00192\nTime:                        18:03:49   Log-Likelihood:                 26629.\nNo. Observations:               50082   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0220      0.001     28.326      0.000       0.021       0.024\ncontrol       -0.0042      0.001     -3.102      0.002      -0.007      -0.002\n==============================================================================\nOmnibus:                    59812.651   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316882.272\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         2.41\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nindependent_vars = df[['intercept', 'control']]\ndependent_var = df['gave']\n\nprobit_model = sm.Probit(dependent_var, independent_vars)\n\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n#The results indicate that being in the control group significantly decreases the likelihood of making a donation compared to the treatment group. \n#The model's overall fit is not particularly strong, suggesting other factors also influence the decision to donate that are not included in the model.\n\nOptimization terminated successfully.\n         Current function value: 0.100445\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50082\nModel:                         Probit   Df Residuals:                    50080\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 16 Apr 2024   Pseudo R-squ.:               0.0009786\nTime:                        18:03:50   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001694\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.0133      0.015   -131.732      0.000      -2.043      -1.983\ncontrol       -0.0868      0.028     -3.113      0.002      -0.141      -0.032\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\ncontrol_group = df[df['ratio']==1]['gave']\ntreatment_group = df[df['ratio']==2]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\ncontrol_group = df[df['ratio']==2]['gave']\ntreatment_group = df[df['ratio']==3]['gave']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\n#Bteween the ratio 1 to 2 and ratio 2 to 3 there is no difference in the amount given.\n\nT-test results -- t-statistic: -0.9660455010162683 p-value: 0.33403190645353387\nT-test results -- t-statistic: -0.04909504833709768 p-value: 0.9608439899712724\n\n\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\ndf['ratio1'] = df['ratio'].apply(lambda x: 1 if x == 1 else 0)\n\nX = df[['intercept', 'ratio1', 'ratio2', 'ratio3']]  # Predictor variables\ny = df['gave']  # Dependent variable\n\nols_model = sm.OLS(y, X)\n\n# Fit the model\nresults = ols_model.fit()\nprint(results.summary())\n\n#Intercept (0.0179): This represents the expected value of gave when all the predictor variables (ratio1, ratio2, ratio3) are 0. The intercept is statistically significant (p &lt; 0.0001), indicating a non-zero baseline level of the dependent variable.\n#Ratio1 (0.0029): The coefficient of ratio1 is 0.0029, suggesting that a one-unit increase in ratio1 is associated with an increase of 0.0029 in the expected value of gave, holding other variables constant. However, this effect is not statistically significant at the typical 0.05 level (p = 0.097), indicating that changes in ratio1 may not reliably predict changes in gave.\n#Ratio2 (0.0048): The coefficient for ratio2 is 0.0048, which is statistically significant (p = 0.006). This suggests that a one-unit increase in ratio2 is associated with an increase of 0.0048 in gave, all else being equal. The result is likely to be a reliable predictor of gave.\n#Ratio3 (0.0049): Similar to ratio2, ratio3 shows a statistically significant positive effect on gave (p = 0.005). Each one-unit increase in ratio3 leads to an increase of 0.0049 in the expected value of gave\n#The standard errors are similar to the ones on the papaer which means The standard error here is almost as large as the coefficient itself (0.0029), indicating a lower level of precision. \n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.666\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0117\nTime:                        18:03:50   Log-Likelihood:                 26629.\nNo. Observations:               50082   AIC:                        -5.325e+04\nDf Residuals:                   50078   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.224      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.746      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59811.123   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316422.352\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.437   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\ncoef_ratio1 = 0.0029  # effect of 1:1\ncoef_ratio2 = 0.0048  # effect of 2:1\ncoef_ratio3 = 0.0049  # effect of 3:1\n\n# Calculate differences in effects from regression coefficients\ncoef_diff_1to1_2to1 = coef_ratio2 - coef_ratio1\ncoef_diff_2to1_3to1 = coef_ratio3 - coef_ratio2\n\nprint(f\"Coefficient difference between 1:1 and 2:1 is {coef_diff_1to1_2to1}\")\nprint(f\"Coefficient difference between 2:1 and 3:1 is {coef_diff_2to1_3to1}\")\n\nCoefficient difference between 1:1 and 2:1 is 0.0018999999999999998\nCoefficient difference between 2:1 and 3:1 is 0.00010000000000000026\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\ncontrol_group = df[df['treatment'] == 0]['amount']\ntreatment_group = df[df['treatment'] == 1]['amount']\n\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n#The difference could be alsmot significznt with 95% confidence howeveer it isnt.\n\nT-test results -- t-statistic: -1.8608252329783106 p-value: 0.06277471702258718\n\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\ncontrol_group = df[(df['treatment'] == 0) & (df['gave'] == 1)]['amount']\ntreatment_group = df[(df['treatment'] == 1) & (df['gave'] == 1)]['amount']\n\n# Drop any NaN values from the amount column for both groups\ncontrol_group = control_group.dropna()\ntreatment_group = treatment_group.dropna()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=True)\n\n# Print the t-test results\nprint(\"T-test results -- t-statistic:\", t_stat, \"p-value:\", p_value)\n\n#Once the gave column is filtered to only times there was a donation the diference is non existent between control and treatment.\n\nT-test results -- t-statistic: 0.5808388615237938 p-value: 0.5614758782284279\n\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nfig, ax = plt.subplots()\nax.hist(df[(df['amount'] &gt; 0) & (df['treatment'] == 0)]['amount'], bins=50)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Donation Amounts for Control Group')\nplt.show()\n\nfig, ax = plt.subplots()\nax.hist(df[(df['amount'] &gt; 0) & (df['treatment'] == 1)]['amount'], bins=50)\nax.axvline(43.87, color='red')\nax.text(43.87 + 1, ax.get_ylim()[1] * 0.9, f'{43.87}', color='red')\nax.set_ylabel('Frequency')\nax.set_xlabel('Donation Amount')\nax.set_title('Donation Amounts for Treatment Group')\nplt.show()"
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nfrom scipy.stats import bernoulli\n\ncontrol = df[df['treatment'] == 0]['gave'].mean()\ntreatment = df[df['treatment'] == 1]['gave'].mean()\n\ncontrol_s = bernoulli.rvs(p = control, size = 10000)\ntreatment_s = bernoulli.rvs(p = treatment, size = 10000)\n\ncum_avg = np.cumsum(treatment_s - control_s) / np.arange(1, 10001)\n\nfig, ax = plt.subplots()\nax.plot(cum_avg)\nax.axhline(treatment - control, color='red')\nax.set_ylabel('Cumulative Average of Differences')\nax.set_xlabel('Draws')\nax.set_title('Cumulative Average')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  }
]